{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Project - Store Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosimo Carlo Canova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To predict sales of different products by store and household based on historical data, including promotional factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Analyze the dataset features: store_nbr, family, onpromotion, sales (target), date\n",
    "### - Identify any relationships between variables, such as the impact of promotions on sales or seasonal differences between stores and products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Time|Content|\n",
    "|---|---|\n",
    "|1|Importing and Loading Data|\n",
    "|2|Data Cleaning and Pre-processing|\n",
    "|3|Exploratory Data Analysis (EDA)|\n",
    "|4|Data Preparation for the Model| \n",
    "|5|Development of the Time Series Model|\n",
    "|6|Model Evaluation|\n",
    "|7|Model Optimization|\n",
    "|8|Implementation and Reporting|\n",
    "|9|Monitoring the Model in Production| \n",
    "|10|Internal Sensitivity Analysis|\n",
    "|11|Development of a Monitoring Dashboard|\n",
    "|12|Cross-Validation| \n",
    "|13|Residual Analysis|\n",
    "|14|Conclusion and Future Work|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/store-sales-time-series-forecasting/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.info())\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, transform categorical variables such as store_nbr and family into numbers or dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=['family','store_nbr'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add useful columns such as: Month, day, week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data['month'] = train_data['date'].dt.month\n",
    "#train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "#train_data['week_of_year'] = train_data['date'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error you're encountering indicates that the date column in your train_data DataFrame is not being recognized as a datetime-like object. Here’s how you can address this issue:\n",
    "\n",
    "Convert the date column to datetime: Make sure the date column is in the correct format by using pd.to_datetime()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['date'] = pd.to_datetime(train_data['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaT values: After conversion, check if there are any NaT values that might indicate conversion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['date'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-apply the .dt accessor: Once the date column is confirmed as a datetime type, you can re-run your original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['week_of_year'] = train_data['date'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['date'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_data['date'], train_data['sales'])  # Replace 'sales' with the column you want to visualize\n",
    "plt.title('Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Plots with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=train_data, x='date', y='sales')  # Replace 'sales' with the column you want to visualize\n",
    "plt.title('Sales Over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=train_data, x='date', y='sales')  # Replace 'sales' with the column you want to visualize\n",
    "plt.title('Sales Over Time')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for infinite values\n",
    "print(np.isinf(train_data).sum())\n",
    "\n",
    "# Check for NaN values\n",
    "print(train_data.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales = train_data.groupby('month')['sales'].sum().reset_index()\n",
    "sns.barplot(data=monthly_sales, x='month', y='sales')\n",
    "plt.title('Monthly Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps: \n",
    "\n",
    "If you want to visualize correlations or data distributed in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = train_data.corr()\n",
    "\n",
    "# Set up a larger figure size\n",
    "plt.figure(figsize=(80, 60))  # Adjust width and height as needed\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the heatmap as a PDF file\n",
    "# plt.savefig('correlation_matrix.pdf', bbox_inches='tight')  # Save as a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives: Understand trends, seasonality, and correlations in the data.\n",
    "\n",
    "Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales distribution: Analyze the distribution of sales globally and by store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade seaborn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Improved histogram plot bwith labels, title, and style\n",
    "sns.histplot(train_data['sales'], bins=50, kde=True, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Sales Distribution', fontsize=16)\n",
    "plt.xlabel('Sales', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "# Add gridlines\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.histplot(): Plots a histogram to show the frequency distribution of sales.\n",
    "\n",
    "bins=50: Specifies the number of bars in the histogram, providing more granularity.\n",
    "\n",
    "kde=True: Adds a smoothed line to estimate the probability density function, which gives a visual sense of the distribution's shape.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "Check for skewness: If sales are heavily skewed, transformations like log scaling might be necessary.\n",
    "\n",
    "Identify if most sales cluster around a certain range or if there are multiple modes (peaks).\n",
    "\n",
    "Look for outliers that may need special attention (e.g., unusually high or low sales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this is important:\n",
    "\n",
    "Analyzing sales over time helps you identify:\n",
    "\n",
    "Seasonality (e.g., whether sales peak at certain times of the year).\n",
    "\n",
    "Trends (e.g., increasing or decreasing sales over time).\n",
    "\n",
    "Anomalies (e.g., sudden spikes or drops that may indicate events like promotions or holidays).\n",
    "\n",
    "What to do:\n",
    "\n",
    "Visualize how total sales evolve over time using a time series plot. Aggregating sales by day, week, or month will help detect long-term patterns.\n",
    "\n",
    "How to do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Group by date and plot total sales over time\n",
    "train_data.groupby('date')['sales'].sum().plot(color='royalblue', linewidth=2)\n",
    "\n",
    "# Add title and labels with better font sizes\n",
    "plt.title('Total Sales Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Total Sales', fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better readability and format as dates\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add gridlines for y-axis\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "train_data.groupby('date')['sales'].sum().plot()\n",
    "plt.title('Sales Over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Promotions on Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this is important:\n",
    "\n",
    "Promotions often have a significant impact on sales.\n",
    "\n",
    "Understanding the relationship between promotions and sales helps you determine how much weight you should give to promotional features in the model. \n",
    "\n",
    "This can lead to better feature engineering or the design of targeted promotional campaigns.\n",
    "\n",
    "What to do:\n",
    "\n",
    "You will use a boxplot to compare sales distributions for items that were on promotion vs. those that were not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot to compare sales with and without promotions\n",
    "sns.boxplot(data=train_data, x='onpromotion', y='sales')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Impact of Promotions on Sales')\n",
    "plt.xlabel('On Promotion (1=Yes, 0=no)')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a larger figure size and Seaborn style\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a boxplot with a more appealing color palette\n",
    "sns.boxplot(data=train_data, x='onpromotion', y='sales', palette='coolwarm')\n",
    "\n",
    "# Add title and labels with better font sizes\n",
    "plt.title('Impact of Promotions on Sales', fontsize=16)\n",
    "plt.xlabel('On Promotion (1 = Yes, 0 = No)', fontsize=14)\n",
    "plt.ylabel('Sales', fontsize=14)\n",
    "\n",
    "# Add gridlines for y-axis for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better fitting\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=train_data, x='onpromotion', y='sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Steps to Enhance EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations Between Features:\n",
    "\n",
    "You can use a heatmap to explore correlations between different numerical features (e.g., sales, onpromotion, store number).\n",
    "\n",
    "This can help identify whether certain stores or product categories have stronger responses to promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_data.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = train_data.select_dtypes(include=[np.number])\n",
    "corr = numeric_data.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Non-Numeric Data: \n",
    "\n",
    "If there are categorical features (e.g., store_nbr, family, etc.) that you want to include in the analysis, consider encoding them before computing the correlation matrix. \n",
    "\n",
    "For example, you could use one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_encoded = pd.get_dummies(train_data, columns=['store_nbr', 'family...'], drop_first=True)\n",
    "# numeric_encoded_data = train_data_encoded.select_dtypes(include=[np.number])\n",
    "# corr = numeric_encoded_data.corr()\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix with Encoded Features')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram. A histogram is useful for understanding the distribution of numerical data.\n",
    "\n",
    "# Example: Visualizing a column named 'sales'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_data['sales'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Sales Distribution')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot. A line plot can show trends over time, especially if the column is indexed by date.\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data['date'], train_data['sales'], color='royalblue', linewidth=2)\n",
    "plt.title('Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot. A box plot is helpful for visualizing the distribution and identifying outliers.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y=train_data['sales'], color='lightgreen')\n",
    "plt.title('Box Plot of Sales')\n",
    "plt.ylabel('Sales')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin Plot. A violin plot combines a box plot with a density plot to show the distribution of the data\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(y=train_data['sales'], color='lightcoral')\n",
    "plt.title('Violin Plot of Sales')\n",
    "plt.ylabel('Sales')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' is in datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Sort the data by date (if not already sorted)\n",
    "train_data = train_data.sort_values('date')\n",
    "\n",
    "# Determine the total number of rows\n",
    "total_rows = len(train_data)\n",
    "\n",
    "# Calculate the number of rows for the training set (80%)\n",
    "train_size = int(total_rows * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows, train_size, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the split date\n",
    "split_date = train_data['date'].iloc[train_size]\n",
    "\n",
    "# Now you can use this split date to create your train and test sets\n",
    "train_set = train_data[train_data['date'] < split_date]\n",
    "test_set = train_data[train_data['date'] >= split_date]\n",
    "\n",
    "print(f'Split Date: {split_date}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = train_data[train_data['date'] < '2017-01-01']\n",
    "#test_set = train_data[train_data['date'] >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Development of the Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines different modeling approaches for forecasting sales based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Time Series Models\n",
    "\n",
    "ARIMA (AutoRegressive Integrated Moving Average)\n",
    "Overview: Suitable for datasets with trends and seasonality.\n",
    "\n",
    "Steps:\n",
    "- Identify parameters p,d and q using ACF and PACF plots \n",
    "- Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identify Parameters p,d and q Using ACF and PACF Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "\n",
    "p: The number of lag observations included in the model (AR term).\n",
    "\n",
    "d: The degree of differencing (to make the series stationary).\n",
    "\n",
    "q: The size of the moving average window (MA term).\n",
    "\n",
    "Steps to Identify Parameters:\n",
    "Check for Stationarity:\n",
    "Before identifying p, d, and q, it’s essential to ensure the time series is stationary. Use the Augmented Dickey-Fuller (ADF) test to check for stationarity\n",
    "\n",
    "Differencing:\n",
    "If the series is not stationary (p-value > 0.05), apply differencing to remove trends and seasonality. This gives you d:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for STATIONARITY:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ACF and PACF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "ACF Plot:\n",
    "Look for where the ACF plot crosses the significance level (usually the dashed lines) and where it cuts off. This indicates the value of q.\n",
    "\n",
    "PACF Plot:\n",
    "Similarly, observe the PACF plot to find where it cuts off to determine p.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all'infinito!\n",
    "\n",
    "#result = adfuller(train_data['sales'])\n",
    "#print('ADF statistic:', result[0])\n",
    "#print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(train_data['sales'].isnull().sum())\n",
    "\n",
    "# Check the type of 'sales' column\n",
    "print(train_data['sales'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformato la colonna sales in numeri \n",
    "train_data['sales'] = pd.to_numeric(train_data['sales'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Handling Missing/Non-Numeric Data: The adfuller function requires a complete numeric series. Missing or non-numeric values will lead to issues, potentially causing the test to run indefinitely.\n",
    "\n",
    "Reducing Dataset Size: If the dataset is large, reducing the size helps test if the function works on a smaller set and gives you feedback on where the issue lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonostante ciò non funziona, quindi nella casella successiva riduco le dimensioni del data set così da rendere il calcolo più veloce.\n",
    "\n",
    "# Running ADF Test on the cleaned 'sales' column\n",
    "#result = adfuller(train_data['sales'])\n",
    "#print('ADF Statistic:', result[0])\n",
    "#print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = train_data.sample(frac=0.1, random_state=42)  # Using 10% of the data\n",
    "result = adfuller(sample_data['sales'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "Stationarity: A p-value of 0.0 means the null hypothesis of the ADF test (which assumes that the series is non-stationary) can be rejected with very high confidence. In other words, the sales series is already stationary.\n",
    "\n",
    "Next Step: Since your data is stationary, there is no need to apply differencing (d = 0). Differencing is only necessary when the series is non-stationary, indicated by a p-value > 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ACF and PACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can proceed with identifying the other parameters for your ARIMA model—p (autocorrelation lag) and q (moving average)—using ACF and PACF plots, since the d parameter is set to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ACF (for q parameter)\n",
    "plt.subplot(121)\n",
    "plot_acf(sample_data['sales'], lags=40, ax=plt.gca())\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF (for p parameter)\n",
    "plt.subplot(122)\n",
    "plot_pacf(sample_data['sales'], lags=40, ax=plt.gca())\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "ACF Plot: Since there is no significant autocorrelation at any lag greater than 0, it indicates that the MA (Moving Average) component, denoted as q, should be 0.\n",
    "\n",
    "PACF Plot: Similarly, the PACF also shows no significant partial autocorrelation beyond lag 0, suggesting that the AR (AutoRegressive) component, denoted as p, should also be 0.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "Both the ACF and PACF plots indicate that there’s no strong autocorrelation in the data, and the model you are working with is likely not benefiting from AR or MA terms. \n",
    "\n",
    "Essentially, this points towards a simple ARIMA(0, d, 0) model, where d is the order of differencing that has been applied or needs to be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the ARIMA model\n",
    "model = ARIMA(train_data['sales'], order=(0, 2, 0))  # Here, p and q are determined from the plots\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring the Model in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Internal Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Development of a Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion and Future Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
